{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn GPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "if not os.path.isfile(\"./datasets/corpora/shakespeare.txt\"):\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > datasets/corpora/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/corpora/shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7b543cb430>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dumb ASCII character-level \"encoding\" since all training data is ASCII\n",
    "def encode_text(text):\n",
    "    return([ord(t) for t in text])\n",
    "\n",
    "def decode_text(indices):\n",
    "    return([chr(x) for x in indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorify data, put it in dataset\n",
    "data = torch.tensor(encode_text(text), dtype=torch.int32)\n",
    "\n",
    "split_idx = int(0.9 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to make a custom PyTorch dataset class to automatically generate the \"context\" windows at load time. This allows us to avoid keeping these windows around in memory when not in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_tensor, context_size):\n",
    "        self.data_tensor = data_tensor\n",
    "        self.context_size = context_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < self.context_size:\n",
    "            x = F.pad(self.data_tensor[:index], (self.context_size - index, 0), value=0)\n",
    "        else:\n",
    "            x = self.data_tensor[index - self.context_size:index]\n",
    "        \n",
    "        y = self.data_tensor[index]\n",
    "        return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE 2023-03-25: I think this is bugged, and that's the reason the training loss is so damn high. Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "---\n",
      "[0, 0, 0, 0, 0, 0, 0, 70]\n",
      "---\n",
      "['F', 'i']\n",
      "Step 1:\n",
      "[0, 0, 0, 0, 0, 0, 70, 105]\n",
      "---\n",
      "[0, 0, 0, 0, 0, 70, 105, 114]\n",
      "---\n",
      "['r', 's']\n",
      "Step 2:\n",
      "[0, 0, 0, 0, 70, 105, 114, 115]\n",
      "---\n",
      "[0, 0, 0, 70, 105, 114, 115, 116]\n",
      "---\n",
      "['t', ' ']\n",
      "Step 3:\n",
      "[0, 0, 70, 105, 114, 115, 116, 32]\n",
      "---\n",
      "[0, 70, 105, 114, 115, 116, 32, 67]\n",
      "---\n",
      "['C', 'i']\n",
      "Step 4:\n",
      "[70, 105, 114, 115, 116, 32, 67, 105]\n",
      "---\n",
      "[105, 114, 115, 116, 32, 67, 105, 116]\n",
      "---\n",
      "['t', 'i']\n",
      "Step 5:\n",
      "[114, 115, 116, 32, 67, 105, 116, 105]\n",
      "---\n",
      "[115, 116, 32, 67, 105, 116, 105, 122]\n",
      "---\n",
      "['z', 'e']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(train_data, 8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "step = 0\n",
    "for x, y in train_dataloader:\n",
    "    print(f\"Step {step}:\")\n",
    "    for b in x.tolist():\n",
    "        print(b)\n",
    "        print(\"---\")\n",
    "\n",
    "    print(decode_text(y.tolist()))\n",
    "    step += 1\n",
    "    if step > 5:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need (注目こそが必要なすべて)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True, device=None, dtype=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        # Save variables\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "\n",
    "        self.Q = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.K = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.V = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        nn.init.kaiming_normal_(self.out_proj.weight, mode='fan_in', nonlinearity='linear')\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Apply linear layers\n",
    "        q = self.Q(query) # [B, C, E]\n",
    "        k = self.K(key) # [B, C, E]\n",
    "        v = self.V(value) # [B, C, E]\n",
    "\n",
    "        # Mutate dimensions so the attention matmul can get rid of the inner d_k\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # [batch_size, num_heads, C, d_k]\n",
    "        k = k.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # [batch_size, num_heads, C, d_k]\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # [batch_size, num_heads, C, d_k]\n",
    "        \n",
    "        # Get raw attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k) # [B, num_heads, C, C]\n",
    "\n",
    "        # Apply mask, if necessary\n",
    "        if key_padding_mask is not None:\n",
    "            \"\"\"\n",
    "            MAY BE WORTH DEBUGGING\n",
    "\n",
    "            if key_padding_mask.dim() == 3:\n",
    "                # If the mask is 3D, add an extra dimension for the num_heads\n",
    "                key_padding_mask = key_padding_mask.unsqueeze(1)  # [batch_size, 1, seq_len, seq_len]\n",
    "            else:\n",
    "                # If the mask is 2D, add dimensions for the num_heads and the 'query' sequence length\n",
    "                key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "            \"\"\"\n",
    "            # Apply the mask to attention scores\n",
    "            scores = scores.masked_fill(key_padding_mask, float('-inf'))\n",
    "\n",
    "        # Scale by sqrt(k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = attn @ v # [B, num_heads, C, d_k]\n",
    "\n",
    "        # Concat and project\n",
    "        # Swap C and num_heads, force memory to coalesce, then fuse back num_heads and d_k together\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        # Project: give attention \"time to think\". Maybe this should be part of a different module but whatever\n",
    "        out = self.out_proj(out)\n",
    "        return(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Self-attention\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mask, dropout=0.2):\n",
    "        super(Block, self).__init__()  \n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.head = MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.ffwd = FeedForward(embed_dim=embed_dim, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connections\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.head.forward(x, x, x, key_padding_mask=self.mask) \n",
    "        out = x + self.ffwd(self.ln2(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, context_size, lr=1e-3):\n",
    "        # Inherit PyTorch stuff\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        # Save variables for later\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = vocab_size\n",
    "        self.context_size = context_size\n",
    "\n",
    "        # Initialize layers. Sadly this breaks the whole \"self.layers: concept but whatever\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embed = nn.Embedding(context_size, embedding_dim)\n",
    "\n",
    "        NUM_HEADS=6\n",
    "        NUM_LAYERS=6\n",
    "        \n",
    "        mask = torch.tril(torch.ones(self.context_size, self.context_size)).bool()\n",
    "        mask = ~mask\n",
    "        self.register_buffer(mask)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(embed_dim=embedding_dim, num_heads=NUM_HEADS, mask=mask) for _ in range(NUM_LAYERS)],\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Final feed-forward layer from embeddings\n",
    "        self.ffwd = nn.Linear(embedding_dim, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_embed = self.tok_embed(x)\n",
    "        tok_embed = tok_embed.view(-1, self.context_size, self.embedding_dim)\n",
    "        pos_embed = self.pos_embed(torch.arange(0, self.context_size, device=\"cuda\")).unsqueeze(0)\n",
    "        x = tok_embed + pos_embed\n",
    "\n",
    "        # The actual attention is all you need here!\n",
    "        # B*C*C cutting out the future\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        preds = self.ffwd(x)\n",
    "        return(preds)\n",
    "    \n",
    "    def infer(self, x):\n",
    "        with torch.no_grad():\n",
    "            res = self.forward(x)\n",
    "            return(res)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, criterion, x, y):\n",
    "    logits = model(x)\n",
    "    last_logits = logits[:, -1, :]\n",
    "    log_probs = nn.LogSoftmax(dim=1)(last_logits)\n",
    "    loss = criterion(log_probs, y.view(-1).long())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_NDIM = 384\n",
    "VOCAB_SIZE = 128\n",
    "BATCH_SIZE=64\n",
    "# \"Context window\"\n",
    "BLOCK_SIZE=256\n",
    "LR=1e-3\n",
    "\n",
    "train_dataset = TextDataset(train_data, BLOCK_SIZE)\n",
    "test_dataset = TextDataset(train_data, BLOCK_SIZE)\n",
    "\n",
    "# Janky training code\n",
    "model = GPT(\n",
    "    embedding_dim=EMBEDDING_NDIM, \n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    context_size=BLOCK_SIZE,\n",
    "    lr=LR\n",
    "    )\n",
    "\n",
    "model = model.to('cuda')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "# TODO Fix this!\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.2)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; loss: 3.3686537742614746\n",
      "Step 100; loss: 3.3535483678181968\n",
      "Step 200; loss: 3.3484479188919067\n",
      "Step 300; loss: 3.344235420227051\n",
      "Step 400; loss: 3.338580369949341\n",
      "Step 500; loss: 3.330465725490025\n",
      "Step 600; loss: 3.333183079957962\n",
      "Step 700; loss: 3.3319032986958823\n",
      "Step 800; loss: 3.332624101638794\n",
      "Step 900; loss: 3.3325188810175117\n",
      "Step 1000; loss: 3.331260542074839\n",
      "Step 1100; loss: 3.3311657355381894\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ritsuko/projects/ai/micrograd/gpt.ipynb Cell 20\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ritsuko/projects/ai/micrograd/gpt.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ritsuko/projects/ai/micrograd/gpt.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ritsuko/projects/ai/micrograd/gpt.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ritsuko/projects/ai/micrograd/gpt.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ritsuko/projects/ai/micrograd/gpt.ipynb#X24sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.\n",
    "EPOCHS = 1\n",
    "STEPS = 5000\n",
    "VAL_INTERVAL = 100\n",
    "\n",
    "losses = []\n",
    "model.train()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=4, shuffle=True)\n",
    "\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for data, target in train_dataloader:\n",
    "        data = data.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "\n",
    "        loss = compute_loss(model, criterion, data, target)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        if step % VAL_INTERVAL == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for x, y in test_dataloader:\n",
    "                    x = x.to(\"cuda\")\n",
    "                    y = y.to(\"cuda\")\n",
    "\n",
    "                    batch_loss = compute_loss(model, criterion, x, y)\n",
    "                    total_loss += batch_loss.item() * 512\n",
    "                    total_samples += 512\n",
    "                    if total_samples > 10:\n",
    "                        break\n",
    "\n",
    "                average_loss = total_loss / total_samples\n",
    "                print(f\"Step {step}; loss: {average_loss}\")\n",
    "                model.train()\n",
    "\n",
    "        step += 1\n",
    "        if step >= STEPS:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"checkpoints/model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store\n",
    "torch.save({\n",
    "    'steps': step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test for overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2399"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4188449382781982\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=4)\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dataloader:\n",
    "        x = x.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "\n",
    "        batch_loss = compute_loss(model, criterion, x, y)\n",
    "        total_loss += batch_loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "        if total_samples > 100:\n",
    "            break\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",n  aon mr\n",
      "nr\n",
      "egtel  s.mangtVk h\n",
      " -hinSfii ol ihIraddeioi akpshaC.n trU d aamooaa eoeEhl:daoUabo'm-fddE auh hpyHs wv'erstiInnmwt hnAuNu ufl\n",
      "I: rl.T   l!eool'lIhl:aynet nna:i yaneehtea hdel\n",
      "  hse l;imi\n",
      "  hgy f iuto eoh gBum.umhemvt\n",
      "a hFo lNsute oaaenh;byeon"
     ]
    }
   ],
   "source": [
    "g_cuda = torch.Generator(device='cuda')\n",
    "\n",
    "contexts = torch.tensor(encode_text(\"God\"), dtype=torch.int32).to('cuda')\n",
    "GEN_LENGTH=256\n",
    "\n",
    "model.eval()\n",
    "for i in range(GEN_LENGTH):\n",
    "    transform = nn.LogSoftmax(1)\n",
    "    # What happens if GEN_LENGTH > CONTEXT? don't worry about it\n",
    "    #x = F.pad(contexts[:, -BLOCK_SIZE:], (0, BLOCK_SIZE - contexts.size(0)), \"constant\", 0)\n",
    "    x = contexts[-BLOCK_SIZE:]\n",
    "    x = F.pad(x, (0, BLOCK_SIZE - x.size(0)), \"constant\", 0).unsqueeze(0) # B*T\n",
    "    preds = model.infer(x)\n",
    "    preds = preds.squeeze(0)\n",
    "    probs = torch.softmax(preds, dim=-1)\n",
    "\n",
    "    # TODO: Broken because of bug with the trailing 0s. FIX THIS\n",
    "    next_char = torch.multinomial(torch.exp(preds[(-1 if i >= BLOCK_SIZE else i), :]), num_samples=1, generator=g_cuda)\n",
    "    #context = torch.cat(context, next_char)\n",
    "    contexts = torch.cat((contexts, next_char), dim=0)\n",
    "    print(decode_text(next_char.cpu().numpy())[-1], end=\"\")\n",
    "\n",
    "#print(\"\".join(decode_text(contexts.cpu().numpy())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
